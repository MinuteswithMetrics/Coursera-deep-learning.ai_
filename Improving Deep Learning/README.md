
# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

https://www.coursera.org/learn/deep-neural-network#

This course will teach you the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. 

The foundations of deep learning are covered in this course:

- Understand industry best-practices for building deep learning applications. 
- Be able to effectively use the common neural network "tricks", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, 
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
- Be able to implement a neural network in TensorFlow. 

This is the second course of the Deep Learning Specialization.

## Content:

### Week 1 -Practical aspects of Deep Learning

- [x] Train / Dev / Test sets
- [x] Bias / Variance
- [x] Basic Recipe for Machine Learning
- [x] Regularization
- [x] Why regularization reduces overfitting?
- [x] Dropout Regularization
- [x] Understanding Dropout
- [x] Other regularization methods
- [x] Normalizing inputs
- [x] Vanishing / Exploding gradients
- [x] Weight Initialization for Deep Networks
- [x] Numerical approximation of gradients
- [x] Gradient checking
- [x] Gradient Checking Implementation Notes

**Practice Questions**
- [x] Quiz: Practical aspects of deep learning 

**Programming Assignments**
- [x] Initialization
- [x] Regularization
- [x] Gradient Checking

---

### Week 2 - Optimization algorithms

- [x] Mini-batch gradient descent
- [x] Understanding mini-batch gradient descent
- [x] Exponentially weighted averages
- [x] Understanding exponentially weighted averages
- [x] Bias correction in exponentially weighted averages
- [x] Gradient descent with momentum
- [x] RMSprop
- [x] Adam optimization algorithm
- [x] Learning rate decay
- [x] The problem of local optima

**Practice Questions**
- [x] Quiz: Optimization algorithms 

**Programming Assignments**
- [x] Optimization


---

### Week 3 - Shallow neural networks

- [x] Tuning process
- [x] Using an appropriate scale to pick hyperparameters
- [x] Hyperparameters tuning in practice: Pandas vs. Caviar
- [x] Normalizing activations in a network
- [x] Fitting Batch Norm into a neural network
- [x] Why does Batch Norm work?
- [x] Batch Norm at test time
- [x] Softmax Regression
- [x] Training a softmax classifier
- [x] Deep learning frameworks
- [x] TensorFlow

**Practice Questions**
- [ ] Quiz: Hyperparameter tuning, Batch Normalization, Programming Frameworks

**Programming Assignment**
- [x] Tensorflow

